"""
AWS ë‹¤ì´ì–´ê·¸ë¨ ì˜¤í† ë¼ë²¨ë§ ëª¨ë“ˆ
AWS ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì—ì„œ AWS ì„œë¹„ìŠ¤ ì•„ì´ì½˜ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ê³  ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- CLIP ê¸°ë°˜ ì•„ì´ì½˜ ìœ ì‚¬ë„ ê²€ìƒ‰
- ORB íŠ¹ì§•ì  ë§¤ì¹­ìœ¼ë¡œ ì •ë°€ë„ í–¥ìƒ
- OCR í…ìŠ¤íŠ¸ íŒíŠ¸ í™œìš©
- NMSë¡œ ì¤‘ë³µ ì œê±°
- ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹ ì§€ì› (YOLO, Label Studio, JSON)
"""

import os
import cv2
import torch
import numpy as np
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
from dataclasses import dataclass
from PIL import Image
from tqdm import tqdm
import faiss
import open_clip

# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from .icon_scanner import IconScanner, IconInfo
from .image_utils import safe_load_image, process_icon_for_clip
from .proposals import propose
from .taxonomy import Taxonomy
from .orb_refine import orb_score
from .ocr_hint import ocr_text
from .exporters import to_labelstudio, to_yolo

@dataclass
class DetectionResult:
    """íƒì§€ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    bbox: List[int]  # [x, y, width, height]
    label: str       # AWS ì„œë¹„ìŠ¤ëª…
    confidence: float # ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
    service_code: str # AWS ì„œë¹„ìŠ¤ ì½”ë“œ

@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    image_path: str
    width: int
    height: int
    detections: List[DetectionResult]
    processing_time: float

class AWSDiagramAutoLabeler:
    """
    AWS ë‹¤ì´ì–´ê·¸ë¨ ì˜¤í† ë¼ë²¨ë§ í´ë˜ìŠ¤
    
    ì‚¬ìš© ì˜ˆì‹œ:
    ```python
    # ì´ˆê¸°í™”
    labeler = AWSDiagramAutoLabeler(
        icons_dir="path/to/aws/icons",
        taxonomy_csv="path/to/taxonomy.csv",
        config={
            "clip_name": "ViT-B-32",
            "clip_pretrained": "openai",
            "detect": {
                "max_size": 1600,
                "canny_low": 60,
                "canny_high": 160,
                "mser_delta": 5,
                "min_area": 900,
                "max_area": 90000,
                "win": 128,
                "stride": 96,
                "iou_nms": 0.45
            },
            "retrieval": {
                "topk": 5,
                "orb_nfeatures": 500,
                "score_clip_w": 0.6,
                "score_orb_w": 0.3,
                "score_ocr_w": 0.1,
                "accept_score": 0.5
            },
            "ocr": {
                "enabled": True,
                "lang": ["en"]
            }
        }
    )
    
    # ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
    result = labeler.analyze_image("path/to/diagram.png")
    
    # ë°°ì¹˜ ë¶„ì„
    results = labeler.analyze_batch(["diagram1.png", "diagram2.png"])
    
    # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
    labeler.export_results(results, "output_dir", format="yolo")
    ```
    """
    
    def __init__(self, icons_dir: str, taxonomy_csv: str, config: Dict):
        """
        AWS ë‹¤ì´ì–´ê·¸ë¨ ì˜¤í† ë¼ë²¨ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            icons_dir: AWS ì•„ì´ì½˜ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
            taxonomy_csv: AWS ì„œë¹„ìŠ¤ íƒì†Œë…¸ë¯¸ CSV íŒŒì¼ ê²½ë¡œ
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # íƒì†Œë…¸ë¯¸ ë¡œë“œ
        # ê°œì„ ëœ íƒì†Œë…¸ë¯¸ ë¡œë“œ (ê·œì¹™ íŒŒì¼ í¬í•¨)
        rules_dir = os.path.join(os.path.dirname(taxonomy_csv), "rules")
        self.taxonomy = Taxonomy.from_csv(taxonomy_csv, rules_dir)
        
        # ì•„ì´ì½˜ ìŠ¤ìºë„ˆ ì´ˆê¸°í™”
        self.icon_scanner = IconScanner(icons_dir, taxonomy_csv)
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        self.model, self.preprocess = self._load_clip_model()
        
        # ì•„ì´ì½˜ ì¸ë±ìŠ¤ ë¹Œë“œ
        self.icons, self.icon_features, self.icon_index = self._build_icon_index()
        
        print(f"âœ… AWS ë‹¤ì´ì–´ê·¸ë¨ ì˜¤í† ë¼ë²¨ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì¥ì¹˜: {self.device}")
        print(f"   - ë¡œë“œëœ ì•„ì´ì½˜: {len(self.icons)}ê°œ")
        print(f"   - íƒì†Œë…¸ë¯¸ ì„œë¹„ìŠ¤: {len(self.taxonomy.names)}ê°œ")
    
    def _load_clip_model(self) -> Tuple[torch.nn.Module, callable]:
        """CLIP ëª¨ë¸ ë¡œë“œ"""
        model, preprocess, _ = open_clip.create_model_and_transforms(
            self.config["clip_name"],
            pretrained=self.config["clip_pretrained"],
            device=self.device
        )
        model.eval()
        return model, preprocess
    
    def _build_icon_index(self) -> Tuple[List[IconInfo], np.ndarray, faiss.Index]:
        """ì•„ì´ì½˜ ì¸ë±ìŠ¤ ë¹Œë“œ"""
        print("ğŸ” AWS ì•„ì´ì½˜ ì¸ë±ìŠ¤ ë¹Œë“œ ì¤‘...")
        
        # ì•„ì´ì½˜ ìŠ¤ìº”
        icons = self.icon_scanner.scan_icons()
        if not icons:
            raise ValueError("ìŠ¤ìº”ëœ AWS ì•„ì´ì½˜ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        # ì„ë² ë”© ìƒì„±
        features = []
        valid_icons = []
        
        for icon in tqdm(icons, desc="ì•„ì´ì½˜ ì„ë² ë”© ìƒì„±"):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
                icon_path = Path(self.icon_scanner.icons_dir) / icon.file_path
                if not icon_path.exists():
                    continue
                
                pil = safe_load_image(str(icon_path))
                if not pil:
                    continue
                
                # CLIP ì „ì²˜ë¦¬
                pil_processed = process_icon_for_clip(pil)
                
                # ì„ë² ë”© ìƒì„±
                feat = self._img_to_feat(pil_processed)
                if feat is not None:
                    features.append(feat)
                    valid_icons.append(icon)
                
            except Exception as e:
                print(f"âš ï¸ ì•„ì´ì½˜ ì²˜ë¦¬ ì‹¤íŒ¨: {icon.file_path} - {e}")
                continue
        
        if not features:
            raise ValueError("ì²˜ë¦¬ëœ ì•„ì´ì½˜ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        features = np.stack(features).astype("float32")
        index = faiss.IndexFlatIP(features.shape[1])
        index.add(features)
        
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(valid_icons)}ê°œ ì•„ì´ì½˜")
        return valid_icons, features, index
    
    def _img_to_feat(self, pil: Image.Image) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ë¥¼ CLIP ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            with torch.no_grad():
                im = self.preprocess(pil).unsqueeze(0).to(self.device)
                f = self.model.encode_image(im)
                f = f / f.norm(dim=-1, keepdim=True)
            return f.squeeze(0).cpu().numpy()
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _search_similar_icons(self, query_feat: np.ndarray, topk: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """ìœ ì‚¬í•œ ì•„ì´ì½˜ ê²€ìƒ‰"""
        D, I = self.icon_index.search(query_feat.reshape(1, -1), topk)
        return D[0], I[0]
    
    def _calculate_scores(self, crop: Image.Image, icon_ref: np.ndarray) -> Tuple[float, float, float]:
        """CLIP, ORB, OCR ì ìˆ˜ ê³„ì‚°"""
        # CLIP ì ìˆ˜
        query_feat = self._img_to_feat(crop)
        if query_feat is None:
            return 0.0, 0.0, 0.0
        
        D, I = self._search_similar_icons(query_feat, 1)
        if len(I) == 0:
            return 0.0, 0.0, 0.0
        
        clip_score = float((D[0] + 1) / 2)
        
        # ORB ì ìˆ˜
        crop_bgr = cv2.cvtColor(np.array(crop), cv2.COLOR_RGB2BGR)
        orb_score_val = orb_score(
            crop_bgr, 
            icon_ref, 
            nfeatures=self.config["retrieval"]["orb_nfeatures"]
        )
        
        # OCR ì ìˆ˜
        ocr_score = 0.0
        if self.config["ocr"]["enabled"]:
            txt = ocr_text(crop, tuple(self.config["ocr"]["lang"]))
            ocr_score = 0.2 if (txt and len(txt) <= 12) else 0.0
        
        return clip_score, orb_score_val, ocr_score
    
    def _nms(self, detections: List[DetectionResult], iou_threshold: float = 0.45) -> List[DetectionResult]:
        """Non-Maximum Suppressionìœ¼ë¡œ ì¤‘ë³µ ì œê±°"""
        if not detections:
            return []
        
        boxes = np.array([d.bbox for d in detections])
        scores = np.array([d.confidence for d in detections])
        
        x1 = boxes[:, 0]
        y1 = boxes[:, 1]
        x2 = boxes[:, 0] + boxes[:, 2]
        y2 = boxes[:, 1] + boxes[:, 3]
        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        
        order = scores.argsort()[::-1]
        keep = []
        
        while order.size > 0:
            i = order[0]
            keep.append(i)
            
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            
            iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
            inds = np.where(iou <= iou_threshold)[0]
            order = order[inds + 1]
        
        return [detections[i] for i in keep]
    
    def analyze_image(self, image_path: str) -> AnalysisResult:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
        
        Args:
            image_path: ë¶„ì„í•  ì´ë¯¸ì§€ ê²½ë¡œ
            
        Returns:
            AnalysisResult: ë¶„ì„ ê²°ê³¼
        """
        import time
        start_time = time.time()
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)
        if img_bgr is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        H, W = img_bgr.shape[:2]
        
        # ê°ì²´ ì œì•ˆ
        boxes = propose(img_bgr, self.config["detect"])
        
        detections = []
        
        for (x, y, w, h) in boxes:
            # ê²½ê³„ í™•ì¸
            x0, y0, x1, y1 = max(0, x), max(0, y), min(W, x + w), min(H, y + h)
            if x1 - x0 < 24 or y1 - y0 < 24:
                continue
            
            # ì´ë¯¸ì§€ í¬ë¡­
            crop = Image.fromarray(cv2.cvtColor(img_bgr[y0:y1, x0:x1], cv2.COLOR_BGR2RGB))
            
            # ìœ ì‚¬í•œ ì•„ì´ì½˜ ê²€ìƒ‰
            query_feat = self._img_to_feat(crop)
            if query_feat is None:
                continue
            
            D, I = self._search_similar_icons(query_feat, self.config["retrieval"]["topk"])
            if len(I) == 0:
                continue
            
            # ìµœì  ë§¤ì¹­ ì•„ì´ì½˜ ì„ íƒ
            best_i = int(I[0])
            if best_i >= len(self.icons):
                continue
            
            icon_info = self.icons[best_i]
            icon_ref = cv2.cvtColor(
                np.array(process_icon_for_clip(
                    safe_load_image(str(Path(self.icon_scanner.icons_dir) / icon_info.file_path))
                )), 
                cv2.COLOR_RGB2BGR
            )
            
            # ì ìˆ˜ ê³„ì‚°
            clip_s, orb_s, ocr_s = self._calculate_scores(crop, icon_ref)
            
            # ê°€ì¤‘í•© ì ìˆ˜
            final_score = (
                self.config["retrieval"]["score_clip_w"] * clip_s +
                self.config["retrieval"]["score_orb_w"] * orb_s +
                self.config["retrieval"]["score_ocr_w"] * ocr_s
            )
            
            # ì„ê³„ê°’ í•„í„°ë§
            if final_score < self.config["retrieval"]["accept_score"]:
                continue
            
            # ë¼ë²¨ ì •ê·œí™”
            label, nsc = self.taxonomy.normalize(icon_info.service_name)
            confidence = round(final_score * 0.7 + nsc * 0.3, 4)
            
            detections.append(DetectionResult(
                bbox=[x0, y0, x1 - x0, y1 - y0],
                label=label,
                confidence=confidence,
                service_code=icon_info.service_code
            ))
        
        # NMS ì ìš©
        if detections:
            detections = self._nms(detections, self.config["detect"]["iou_nms"])
            detections = sorted(detections, key=lambda d: -d.confidence)
        
        processing_time = time.time() - start_time
        
        return AnalysisResult(
            image_path=image_path,
            width=W,
            height=H,
            detections=detections,
            processing_time=processing_time
        )
    
    def analyze_batch(self, image_paths: List[str]) -> List[AnalysisResult]:
        """
        ë°°ì¹˜ ì´ë¯¸ì§€ ë¶„ì„
        
        Args:
            image_paths: ë¶„ì„í•  ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[AnalysisResult]: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for image_path in tqdm(image_paths, desc="ë°°ì¹˜ ë¶„ì„"):
            try:
                result = self.analyze_image(image_path)
                results.append(result)
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {image_path} - {e}")
                continue
        return results
    
    def export_results(self, results: List[AnalysisResult], output_dir: str, 
                      format: str = "json") -> str:
        """
        ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
            format: ì¶œë ¥ í˜•ì‹ ("json", "yolo", "labelstudio")
            
        Returns:
            str: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        os.makedirs(output_dir, exist_ok=True)
        
        if format == "json":
            # JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
            output_data = []
            for result in results:
                output_data.append({
                    "image_path": result.image_path,
                    "width": result.width,
                    "height": result.height,
                    "processing_time": result.processing_time,
                    "detections": [
                        {
                            "bbox": d.bbox,
                            "label": d.label,
                            "confidence": d.confidence,
                            "service_code": d.service_code
                        }
                        for d in result.detections
                    ]
                })
            
            output_path = os.path.join(output_dir, "detections.json")
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            return output_path
        
        elif format == "yolo":
            # YOLO í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
            yolo_data = []
            for result in results:
                yolo_data.append({
                    "image_path": result.image_path,
                    "width": result.width,
                    "height": result.height,
                    "objects": [
                        {
                            "bbox": d.bbox,
                            "label": d.label,
                            "score": d.confidence
                        }
                        for d in result.detections
                    ]
                })
            
            yolo_dir = os.path.join(output_dir, "yolo")
            to_yolo(yolo_data, yolo_dir)
            return yolo_dir
        
        elif format == "labelstudio":
            # Label Studio í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
            ls_data = []
            for result in results:
                ls_data.append({
                    "image_path": result.image_path,
                    "width": result.width,
                    "height": result.height,
                    "objects": [
                        {
                            "bbox": d.bbox,
                            "label": d.label,
                            "score": d.confidence
                        }
                        for d in result.detections
                    ]
                })
            
            ls_json = to_labelstudio(ls_data)
            output_path = os.path.join(output_dir, "labelstudio.json")
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(ls_json, f, ensure_ascii=False, indent=2)
            
            return output_path
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
    
    def get_statistics(self, results: List[AnalysisResult]) -> Dict:
        """ë¶„ì„ ê²°ê³¼ í†µê³„"""
        total_images = len(results)
        total_detections = sum(len(r.detections) for r in results)
        avg_processing_time = np.mean([r.processing_time for r in results])
        
        # ì„œë¹„ìŠ¤ë³„ í†µê³„
        service_counts = {}
        for result in results:
            for detection in result.detections:
                service_counts[detection.label] = service_counts.get(detection.label, 0) + 1
        
        return {
            "total_images": total_images,
            "total_detections": total_detections,
            "avg_processing_time": avg_processing_time,
            "detection_rate": total_detections / total_images if total_images > 0 else 0,
            "service_distribution": service_counts
        }
